{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Your Title\"\n",
        "format: \n",
        "  pdf:\n",
        "    keep-tex: true\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "  python: \"/opt/anaconda3/envs/VSCODE/bin/python\"\n",
        "---\n",
        "\n",
        "\n",
        "**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. \n",
        "We use (`*`) to indicate a problem that we think might be time consuming. \n",
        "    \n",
        "## Style Points (10 pts) \n",
        "Please refer to the minilesson on code style\n",
        "**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.\n",
        "\n",
        "## Submission Steps (10 pts)\n",
        "1. This problem set is a paired problem set.\n",
        "2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.\n",
        "    - Partner 1 (name and cnet ID):\n",
        "    - Partner 2 (name and cnet ID):\n",
        "3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. \n",
        "4. \"This submission is our work alone and complies with the 30538 integrity policy.\" Add your initials to indicate your agreement: \\*\\*\\_\\_\\*\\* \\*\\*\\_\\_\\*\\*\n",
        "5. \"I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**\"  (1 point)\n",
        "6. Late coins used this pset: \\*\\*\\_\\_\\*\\* Late coins left after submission: \\*\\*\\_\\_\\*\\*\n",
        "7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, \n",
        "    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. \n",
        "8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.\n",
        "9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.\n",
        "10. (Partner 1): tag your submission in Gradescope\n",
        "\n",
        "**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. \n"
      ],
      "id": "51b97232"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "alt.renderers.enable(\"png\")"
      ],
      "id": "11ba04e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download and explore the Provider of Services (POS) file (10 pts)\n",
        "\n",
        "1. \n",
        "```\n",
        "PRVDR_CTGRY_SBTYP_CD = Sub-type of Provider\n",
        "PRVDR_CTGRY_CD = Provider Category Code\n",
        "PRVDR_NUM = CMS Certification Number\n",
        "PGM_TRMNTN_CD = Termination Code\n",
        "TRMNTN_EXPRTN_DT = Date the provider was terminated \n",
        "FAC_NAME = Facility Name\n",
        "ZIP_CD = Zip code\n",
        "STATE_CD = State Abbreviation\n",
        "```\n",
        "\n",
        "2. \n",
        "    a.\n"
      ],
      "id": "cd20376f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# set path and read in the pos2016.csv file\n",
        "pset_path = '/Users/aa/Documents/GitHub/problem-set-4-katika-and-liujun/data/'\n",
        "\n",
        "# create a function to load, filter, and count number of short-term hospitals\n",
        "def read_short_term_hospitals(year, pset_path):\n",
        "    \"\"\"\n",
        "    read the provider-of-service data for a given year, filters for short-term hospitals,\n",
        "    and returns the filtered DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        year (int): The year of the data (e.g., 2017).\n",
        "        pset_path (str): The path to the directory containing the CSV files.\n",
        "    \"\"\"\n",
        "    file_path = os.path.join(pset_path, f'pos{year}.csv')\n",
        "    df = pd.read_csv(file_path, encoding='latin1')\n",
        "    df = df[(df['PRVDR_CTGRY_CD'] == 1) & (df['PRVDR_CTGRY_SBTYP_CD'] == 1)]\n",
        "    print(\n",
        "        f'There are {df.shape[0]} short-term hospitals reported in {year} data')\n",
        "    return df\n",
        "\n",
        "# import pos_2016.csv\n",
        "df_pos2016 = read_short_term_hospitals(2016, pset_path)"
      ],
      "id": "cfaa7596",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    b.   \n",
        "    According to the American Hospital Association (https://www.aha.org/statistics/2018-01-09-fast-facts-us-hospitals-2018-pie-charts), there were 5,534 hospitals in the U.S. in total which is much lower than 7,245 considering that our data only consists of short-term hospitals. One reason could be due to the difference in definition of a hospital from each source.\n",
        "\n",
        "3.  "
      ],
      "id": "e34a0274"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# repeat the steps for each year\n",
        "df_pos2017 = read_short_term_hospitals(2017, pset_path)\n",
        "df_pos2018 = read_short_term_hospitals(2018, pset_path)\n",
        "df_pos2019 = read_short_term_hospitals(2019, pset_path)\n",
        "\n",
        "# append all the pos data\n",
        "df_pos2016_to_2019 = pd.concat([df_pos2016.assign(year=2016), df_pos2017.assign(\n",
        "    year=2017), df_pos2018.assign(year=2018), df_pos2019.assign(year=2019)], ignore_index=True)"
      ],
      "id": "ee07e4c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot number of observations by year\n",
        "observation_by_year = df_pos2016_to_2019.groupby(\n",
        "    'year').size().reset_index(name='observation_count')\n",
        "\n",
        "observation_by_year_plot = alt.Chart(observation_by_year).mark_bar(size=20).encode(\n",
        "    alt.X('year:O', title='Year'),\n",
        "    alt.Y('observation_count:Q', title='Number of Observations'),\n",
        ").properties(\n",
        "    title='Number of observations im Provide-of-Service file between 2016 to 2019',\n",
        "    width=300,\n",
        "    height=300\n",
        ")\n",
        "\n",
        "observation_label = observation_by_year_plot.mark_text(\n",
        "    align='left',\n",
        "    baseline='bottom',\n",
        "    dx=-7,\n",
        "    dy=-5,\n",
        "    fontSize=10\n",
        ").encode(\n",
        "    text='observation_count:Q'\n",
        ")\n",
        "\n",
        "observation_by_year_plot + observation_label"
      ],
      "id": "f515d224",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.  \n",
        "    a. Plot number of unique hospitals by year"
      ],
      "id": "f464ef5f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "unique_hospital_by_year = df_pos2016_to_2019.groupby(\n",
        "    'year')['PRVDR_NUM'].nunique().reset_index(name='unique_hospital_count')\n",
        "\n",
        "unique_hospital_by_year_plot = alt.Chart(unique_hospital_by_year).mark_bar(size=20, color='lightcoral').encode(\n",
        "    alt.X('year:O', title='Year'),\n",
        "    alt.Y('unique_hospital_count:Q', title='Number of unique hospitals')\n",
        ").properties(\n",
        "    title='Total unique hospitals by year',\n",
        "    width=300,\n",
        "    height=300\n",
        ")\n",
        "\n",
        "unique_label = unique_hospital_by_year_plot.mark_text(\n",
        "    align='left',\n",
        "    baseline='bottom',\n",
        "    dx=-5,\n",
        "    dy=-5,\n",
        "    fontSize=10\n",
        ").encode(\n",
        "    text='unique_hospital_count:Q'\n",
        ")\n",
        "\n",
        "unique_hospital_by_year_plot + unique_label"
      ],
      "id": "84784664",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    b. "
      ],
      "id": "71520b51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "observation_by_year_plot + observation_label  | unique_hospital_by_year_plot + unique_label"
      ],
      "id": "440a4352",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the two plots, we can see that the number of unique hospitals is exactly the same as the number of observations in each year. It tells us that a hospital only appears in the data once in a specific year, that is each row in each year represents a snapshot of a unique hospital.\n",
        "\n",
        "## Identify hospital closures in POS file (15 pts) (*)\n",
        "\n",
        "1. "
      ],
      "id": "33391810"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = df_pos2016_to_2019.copy()\n",
        "\n",
        "# initialize a dictionary to store termination years\n",
        "termination_years = {}\n",
        "\n",
        "# extract the active hospitals in 2016\n",
        "active_2016 = df[(df['year'] == 2016) & (df['PGM_TRMNTN_CD'] == 0)]\n",
        "\n",
        "active_df = df[df['PRVDR_NUM'].isin(active_2016['PRVDR_NUM'])]\n",
        "\n",
        "# group data by CMS code\n",
        "for hospital, group in active_df.groupby('PRVDR_NUM'):\n",
        "    # sort records by year for this hospital\n",
        "    group = group.sort_values('year')\n",
        "\n",
        "    # set a default termination year as None\n",
        "    termination_year = None\n",
        "\n",
        "    # check each year from 2017 to 2019 for termination\n",
        "    for year in [2017, 2018, 2019]:\n",
        "        # filter the data for the current year\n",
        "        yearly_data = group[group['year'] == year]\n",
        "\n",
        "        # if no record for the hospital in this year, mark as terminated\n",
        "        if yearly_data.empty:\n",
        "            termination_year = year\n",
        "            break\n",
        "        # if the hospital is present but not active, mark as terminated\n",
        "        elif yearly_data['PGM_TRMNTN_CD'].values[0] != 0:\n",
        "            termination_year = year\n",
        "            break\n",
        "\n",
        "    # if a termination year was identified, store it in the dictionary\n",
        "    if termination_year:\n",
        "        termination_years[hospital] = termination_year\n",
        "\n",
        "# convert the termination years to a DataFrame\n",
        "terminate_year_df = pd.DataFrame(list(termination_years.items()), columns=[\n",
        "                                 'PRVDR_NUM', 'Termination_Year'])\n",
        "\n",
        "# adding ZIP code information for each hospital from original dataset\n",
        "closed_hospitals_zipcode = active_df[active_df['year'] == 2016][[\n",
        "    'PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].drop_duplicates('PRVDR_NUM')\n",
        "\n",
        "# combine termination years with ZIP code information\n",
        "closed_hospitals = terminate_year_df.merge(\n",
        "    closed_hospitals_zipcode, on='PRVDR_NUM', how='left')"
      ],
      "id": "cfe5d526",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    f\"{closed_hospitals.shape[0]} hospitals were active in 2016 that were suspected to have closed by 2019\")"
      ],
      "id": "a05d508a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. "
      ],
      "id": "65da17df"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "closed_hospitals.sort_values('FAC_NAME')[['FAC_NAME','Termination_Year']].head(10)"
      ],
      "id": "5bec2ee3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. "
      ],
      "id": "9af6356e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# count the active hospitals of each zip code every year\n",
        "\n",
        "active_counts = active_df.groupby(['year', 'ZIP_CD'])['PGM_TRMNTN_CD'].apply(\n",
        "    lambda x: (x == 0).sum()).reset_index(name='active_count')\n",
        "\n",
        "zip_and_close_year = pd.DataFrame(closed_hospitals.groupby(\n",
        "    ['ZIP_CD', 'Termination_Year']).size()).reset_index()\n",
        "\n",
        "zip_with_merger = []\n",
        "\n",
        "for index, row in zip_and_close_year.iterrows():\n",
        "    zip = row['ZIP_CD']\n",
        "    year = row['Termination_Year']\n",
        "    if year == 2019:\n",
        "        continue\n",
        "\n",
        "    zip_data = active_counts[active_counts['ZIP_CD'] == zip]\n",
        "    count_this_year = zip_data[zip_data['year'] == year]['active_count']\n",
        "    count_next_year = zip_data[zip_data['year'] == (year + 1)]['active_count']\n",
        "\n",
        "    if not count_this_year.empty and not count_next_year.empty:\n",
        "        # extract scalar values (there should be only one value per year now)\n",
        "        count_this_year = count_this_year.iloc[0]\n",
        "        count_next_year = count_next_year.iloc[0]\n",
        "\n",
        "        if count_this_year <= count_next_year:\n",
        "            zip_with_merger.append(zip)"
      ],
      "id": "e04fbe10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    a. How many hospitals fit this definition of potentially being a merger/acquisition?"
      ],
      "id": "1f896760"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"{len(zip_with_merger)} hospitals fit this definition of potentially being a merger/acquisition.\")"
      ],
      "id": "80749e41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    b. After correcting for this, how many hospitals do you have left?"
      ],
      "id": "54b9af6a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# filter merger hospitials\n",
        "closed_hospitals = closed_hospitals[~closed_hospitals['ZIP_CD'].isin(\n",
        "    zip_with_merger)]\n",
        "\n",
        "print(\n",
        "    f\"{closed_hospitals.shape[0]} hospitals were active in 2016 that were suspected to have closed by 2019\")"
      ],
      "id": "467bd14b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    c. "
      ],
      "id": "6da0dbd3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "closed_hospitals.sort_values(\n",
        "    'FAC_NAME')[['FAC_NAME', 'Termination_Year']].head(10)"
      ],
      "id": "3ba7ace6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Census zip code shapefile (10 pt) \n",
        "\n",
        "1. \n",
        "    a.\n",
        "| File Type | Information Type |\n",
        "| .dbf| Attribute information |\n",
        "| .prj | Coordinate Reference System (CRS) description |\n",
        "| .shp | Geometric or Spatial data (i.e., points, polygon) |\n",
        "| .shx | Positional index for looking up geometries |\n",
        "| .xml | Metadata about the dataset including descriptions, purpose, agreement of use etc. |\n",
        "\n",
        "    b. "
      ],
      "id": "193d8fd5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# zip_path = '/Users/katikaklinkaew/Documents/data/gz_2010_us_860_00_500k'\n",
        "zip_path = '/Users/aa/Documents/Q4/Python/shp'\n",
        "\n",
        "# Print the size of each file in the folder\n",
        "for dataset in os.listdir(zip_path):\n",
        "    file_path = os.path.join(zip_path, dataset)\n",
        "    size = os.path.getsize(file_path)\n",
        "    if size < 1024:\n",
        "        size_str = f\"{size:.2f} Bytes\"\n",
        "    elif size < 1024 ** 2:\n",
        "        size_str = f\"{size/1024:.2f} KB\"\n",
        "    else:\n",
        "        size_str = f\"{size/(1024 ** 2):.2f} MB\"\n",
        "\n",
        "    print(f\"{dataset}: {size_str}\")"
      ],
      "id": "960645e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. "
      ],
      "id": "d329236a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "shapefile_path = os.path.join(zip_path, 'gz_2010_us_860_00_500k.shp')\n",
        "zip_shp = gpd.read_file(shapefile_path)"
      ],
      "id": "659d8d68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Texas zip codes start with 733, and 750 - 799\n",
        "zip_tx = zip_shp[zip_shp['NAME'].str.startswith(\n",
        "    ('733', '75', '76', '77', '78', '79'))]\n",
        "\n",
        "active_2016['ZIP_CD'] = active_2016['ZIP_CD'].astype(\n",
        "    int).astype(str).str.zfill(5)\n",
        "zip_tx['NAME'] = zip_tx['NAME'].astype(str).str.zfill(5)\n",
        "\n",
        "tx_hospitals = active_2016[active_2016['ZIP_CD'].isin(zip_tx['NAME'])]\n",
        "\n",
        "tx_hospitals_zip = tx_hospitals.groupby(\n",
        "    'ZIP_CD').size().reset_index(name='Number of Hospitals')\n",
        "\n",
        "zip_tx = zip_tx.rename(columns={'NAME': 'ZIP_CD'})\n",
        "\n",
        "# Merge on ZIP_CD to get Texas ZIP codes with hospital counts\n",
        "tx_hospitals_merged = zip_tx.merge(tx_hospitals_zip, on='ZIP_CD', how='left')\n",
        "\n",
        "tx_hospitals_merged['Number of Hospitals'] = tx_hospitals_merged['Number of Hospitals'].fillna(\n",
        "    0)"
      ],
      "id": "3cf2d76d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tx_hospitals_merged[tx_hospitals_merged['Number of Hospitals']\n",
        "                    == tx_hospitals_merged['Number of Hospitals'].max()]"
      ],
      "id": "5afcde39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "tx_hospitals_zip_plot = tx_hospitals_merged.plot(\n",
        "    column='Number of Hospitals',\n",
        "    legend=True,\n",
        "    linewidth=0.5).set_axis_off()\n",
        "plt.title('Number of Hospitals by ZIP Codes in Texas in 2016')\n",
        "plt.show()"
      ],
      "id": "a593a867",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate zip code’s distance to the nearest hospital (20 pts) (*)\n",
        "\n",
        "1. "
      ],
      "id": "da04bbaf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#  Create a GeoDataFrame for the centroid of each zip code nationally\n",
        "zips_all_centroids = gpd.GeoDataFrame({\n",
        "    'ZIP_CD': zip_shp['ZCTA5'],\n",
        "    'centroid': zip_shp.geometry.centroid\n",
        "})\n",
        "\n",
        "zips_all_centroids.shape"
      ],
      "id": "78b26526",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dimensions of the GeoDataFrame include 33120 rows and 2 columns. The first column 'ZIP_CD' is the zip codes nationally and the second column 'centroid' is the position of all the points in the zip code polygons.\n",
        "\n",
        "2. "
      ],
      "id": "bf2ddb92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZIP_CD'].str.startswith(\n",
        "    ('733', '75', '76', '77', '78', '79'))]\n",
        "\n",
        "# the border states include NM, OK, AR, LA\n",
        "# zip codes start with 700 - 749, 870 - 884\n",
        "\n",
        "zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids['ZIP_CD'].str.startswith(\n",
        "    ('7', '87', '88'))]\n",
        "\n",
        "print(\n",
        "    f\"Unique zip codes in zip_texas_centroids: {zips_texas_centroids['ZIP_CD'].nunique()}\")\n",
        "print(\n",
        "    f\"Unique zip codes in zips_texas_borderstates_centroids: {zips_texas_borderstates_centroids['ZIP_CD'].nunique()}\")"
      ],
      "id": "a83b6149",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. I will do a inner join merge on variables merging on ZIP_CD."
      ],
      "id": "e2c0a8cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "zip_2016 = pd.DataFrame(active_2016['ZIP_CD'].fillna(\n",
        "    0).astype(int).astype(str).str.zfill(5).drop_duplicates())\n",
        "zip_2016.columns = ['ZIP_CD']\n",
        "zips_texas_borderstates_centroids['ZIP_CD'] = zips_texas_borderstates_centroids['ZIP_CD'].astype(\n",
        "    str)\n",
        "\n",
        "zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(\n",
        "    zip_2016, on='ZIP_CD')"
      ],
      "id": "46fdc0c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "zips_withhospital_centroids.head(1)"
      ],
      "id": "410af12c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \n",
        "    a. Try the join with 10 zip codes"
      ],
      "id": "24c8379d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "from shapely.geometry import Point\n",
        "\n",
        "zips_texas_centroids = zips_texas_centroids.set_geometry('centroid')\n",
        "zips_withhospital_centroids = zips_withhospital_centroids.set_geometry(\n",
        "    'centroid')\n",
        "\n",
        "# subset the first ten row\n",
        "zips_texas_centroids_subset = zips_texas_centroids[:10]\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "nearest_distances = []\n",
        "\n",
        "# loop over each row in the first 10 rows of zips_texas_centroids\n",
        "for _, row_tx in zips_texas_centroids_subset.iterrows():\n",
        "    point1 = row_tx['centroid']\n",
        "    nearest_distance = float('inf')  # Initialize to a large number\n",
        "\n",
        "    # loop over each row in zips_withhospital_centroids\n",
        "    for _, row_all in zips_withhospital_centroids.iterrows():\n",
        "        point2 = row_all['centroid']\n",
        "\n",
        "        # calculate distance\n",
        "        distance_new = point1.distance(point2)\n",
        "\n",
        "        # update nearest distance if a closer point is found\n",
        "        if distance_new < nearest_distance:\n",
        "            nearest_distance = distance_new\n",
        "\n",
        "    # append the nearest distance for the current row in zips_texas_centroids\n",
        "    nearest_distances.append(nearest_distance)\n",
        "\n",
        "# assign the calculated distances\n",
        "zips_texas_centroids_subset['nearest_distance'] = nearest_distances\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Runtime: {end_time - start_time}\")"
      ],
      "id": "58dffbab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It takes around 0.13 seconds to join the 10 zip codes. As there are 1935 unique zip codes in zip_texas_centroids, the total would be 0.13*1935/10 = 25 seconds. In other words, we estimate the entire procedure will take around 25 seconds.\n",
        "\n",
        "    b. Doing the full join"
      ],
      "id": "c26284a5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "start_time = time.time()\n",
        "\n",
        "nearest_distances = []\n",
        "\n",
        "# loop over each row in zips_texas_centroids\n",
        "for _, row_tx in zips_texas_centroids.iterrows():\n",
        "    point1 = row_tx['centroid']\n",
        "    nearest_distance = float('inf')  # initialize to a large number\n",
        "\n",
        "    # loop over each row in zips_withhospital_centroids\n",
        "    for _, row_all in zips_withhospital_centroids.iterrows():\n",
        "        point2 = row_all['centroid']\n",
        "\n",
        "        # calculate distance\n",
        "        distance_new = point1.distance(point2)\n",
        "\n",
        "        # update nearest distance if a closer point is found\n",
        "        if distance_new < nearest_distance:\n",
        "            nearest_distance = distance_new\n",
        "\n",
        "    # append the nearest distance for the current row in zips_texas_centroids\n",
        "    nearest_distances.append(nearest_distance)\n",
        "\n",
        "# assign the calculated distances\n",
        "zips_texas_centroids['nearest_distance'] = nearest_distances\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Runtime: {end_time - start_time}\")"
      ],
      "id": "e0308123",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    c.\n",
        "    The unit is 'Degree'. One degree of latitude is approximately 69 miles, while one degree of longitude is approximately 54.6 miles. In this case, we will neglect longitude and multiply the degree by 69 to convert it to miles.\n"
      ],
      "id": "ad85c7f4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# convert the degree unit to miles\n",
        "zips_texas_centroids['nearest_distance'] = zips_texas_centroids['nearest_distance'] * 69"
      ],
      "id": "c19c92b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Calculate the average distance to the nearest hospital for each zip code in Texas\n",
        "\n",
        "    a. The unit is 'Miles'.\n",
        "\n",
        "    b. Report the average distance in miles"
      ],
      "id": "e3533cb9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "zips_texas_centroids['nearest_distance'].mean()"
      ],
      "id": "e3fa9ac7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The average distance to the nearest hospital for each zip code in Texas is 14.56 miles, which makes sense.\n",
        "\n",
        "    c. Map the value for each zip code"
      ],
      "id": "7b7822be"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plotting the polygons from zip_tx\n",
        "zip_tx.plot(ax=ax, color=None, edgecolor=\"grey\", alpha=0.5, label=\"Dense\")\n",
        "\n",
        "# Plotting the centroids with a color gradient based on 'nearest_distance'\n",
        "# `cmap` sets the color map (e.g., 'viridis', 'coolwarm', 'plasma', etc.)\n",
        "zips_texas_centroids.plot(\n",
        "    ax=ax,\n",
        "    column='nearest_distance',\n",
        "    cmap='YlGnBu',  # Choose a color map that suits your visualization\n",
        "    alpha=0.8,\n",
        "    markersize=20,\n",
        "    legend=True,  # Show legend for color scale\n",
        "    label=\"Centroids\"\n",
        ")\n",
        "\n",
        "# Remove axis for a cleaner look\n",
        "ax.set_axis_off()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "id": "7e2e81d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most areas on the east side of Texas have nearby hospitals, while some areas on the west side near the border have a much greater distance to the nearest hospital.\n",
        "\n",
        "\n",
        "## Effects of closures on access in Texas (15 pts)\n",
        "\n",
        "1. "
      ],
      "id": "5dd889e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "closed_hospitals['ZIP_CD'] = closed_hospitals['ZIP_CD'].astype(\n",
        "    int).astype(str).str.zfill(5)\n",
        "\n",
        "# filter to hospital closures in Texas\n",
        "closed_hospitals_tx = closed_hospitals[closed_hospitals['ZIP_CD'].isin(\n",
        "    zip_tx['ZIP_CD'])]\n",
        "\n",
        "# count number of hospital closures by each zip code\n",
        "closed_hospitals_tx_zip = closed_hospitals_tx.groupby(\n",
        "    'ZIP_CD').size().reset_index(name='Number of Hospital Closures')\n",
        "\n",
        "closed_hospitals_tx_zip"
      ],
      "id": "fbdba6df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. "
      ],
      "id": "9be33403"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# merge hospital closures in Texas to Texas shapefile\n",
        "closed_hospitals_tx_zip_merged = zip_tx.merge(\n",
        "    closed_hospitals_tx_zip, on='ZIP_CD', how='left')\n",
        "\n",
        "# replace na with 0\n",
        "closed_hospitals_tx_zip_merged['Number of Hospital Closures'] = closed_hospitals_tx_zip_merged['Number of Hospital Closures'].fillna(\n",
        "    0)\n",
        "\n",
        "closed_hospitals_tx_zip_merged['directly affected'] = (\n",
        "    closed_hospitals_tx_zip_merged['Number of Hospital Closures'] > 0).astype(int)\n",
        "directly_affected_zip_count = closed_hospitals_tx_zip_merged['directly affected'].sum(\n",
        ")\n",
        "\n",
        "print('There are', directly_affected_zip_count,\n",
        "      'directly affected zip codes in Texas')"
      ],
      "id": "bcad2c01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# plot a choropleth with directly affected zip codes in red and others in lightgray\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "closed_hospitals_tx_zip_merged.plot(\n",
        "    column='directly affected',\n",
        "    color=closed_hospitals_tx_zip_merged['directly affected'].map(\n",
        "        {0: 'lightgray', 1: 'red'}),  # Single color for affected areas\n",
        "    edgecolor='0.8',\n",
        "    legend=True,\n",
        "    ax=ax\n",
        ")\n",
        "# create custom legends\n",
        "affected_patch = mpatches.Patch(color='red', label='Directly Affected')\n",
        "not_affected_patch = mpatches.Patch(\n",
        "    color='lightgray', label='Not Directly Affected')\n",
        "plt.legend(handles=[not_affected_patch, affected_patch],\n",
        "           loc='upper right', title=\"Closure Impact Status\")\n",
        "\n",
        "ax.set_axis_off()\n",
        "plt.title('Texas ZIP Codes directly affected by hospital closure in 2016 - 2019')\n",
        "plt.show()"
      ],
      "id": "1ae223f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. "
      ],
      "id": "1aba289e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create a GeoDataFrame of the directly affected zip codes\n",
        "directly_affected_zips = closed_hospitals_tx_zip_merged[closed_hospitals_tx_zip_merged['directly affected'] == 1].copy()\n",
        "\n",
        "directly_affected_zips = directly_affected_zips[['ZIP_CD', 'geometry']]\n",
        "\n",
        "print('Check if the object is a GepDataFrame:', type(directly_affected_zips))"
      ],
      "id": "66630dfe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# geopandas uses meters to creat buffer, check the unit for our GeoDataFrame\n",
        "directly_affected_zips.crs\n",
        "# create a list of directly affected zip codes\n",
        "directly_affected_zips_list = directly_affected_zips['ZIP_CD'].unique().tolist()\n",
        "\n",
        "# create a copy and convert into crs for Texas that is in meters\n",
        "directly_affected_buffer = directly_affected_zips.copy()\n",
        "directly_affected_buffer = directly_affected_buffer.to_crs(epsg=3083)\n",
        "\n",
        "# create a 10-mile buffer by converting into meters\n",
        "directly_affected_buffer['10-mile radius'] = directly_affected_buffer.geometry.buffer(\n",
        "    10*1609.34)\n",
        "directly_affected_buffer = directly_affected_buffer.set_geometry(\n",
        "    '10-mile radius')"
      ],
      "id": "2e7a1874",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# before doing spatial join, ensure the overall Texas ZIP shapefile is in the same crs\n",
        "zip_tx = zip_tx.to_crs(epsg=3083)\n",
        "\n",
        "# do the spatial join which will return all directly and indirectly affected zip codes\n",
        "indirectly_affected_zips = gpd.sjoin(zip_tx, directly_affected_buffer,\n",
        "                                     how=\"inner\", predicate='intersects')\n",
        "\n",
        "# rename ZIP_CD_left to 'ZIP_CD' for indirectly affected zips GeoDataFrame and set geometry back to geometry\n",
        "indirectly_affected_zips = indirectly_affected_zips.rename(\n",
        "    columns={'ZIP_CD_left': 'ZIP_CD'})\n",
        "\n",
        "# create a list of only indirectly affected zip codes\n",
        "indirectly_affected_zips_list = indirectly_affected_zips['ZIP_CD'].unique().tolist()\n",
        "\n",
        "indirectly_affected_zips_list = [\n",
        "    zip_code for zip_code in indirectly_affected_zips_list\n",
        "    if zip_code not in directly_affected_zips_list\n",
        "]\n",
        "\n",
        "print('There are', len(indirectly_affected_zips_list),\n",
        "      'indirectly affected zip codes in Texas')\n",
        "\n",
        "# set geometry back to the original geometry column\n",
        "indirectly_affected_zips = indirectly_affected_zips.set_geometry('geometry')"
      ],
      "id": "d72a5711",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. "
      ],
      "id": "03108b22"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create a column representing Hospital Closures Impact Status\n",
        "closed_hospitals_tx_zip_merged['Closure Impact Status'] = 'Not Affected'\n",
        "closed_hospitals_tx_zip_merged.loc[closed_hospitals_tx_zip_merged['ZIP_CD'].isin(\n",
        "    directly_affected_zips_list), 'Closure Impact Status'] = 'Directly Affected'\n",
        "closed_hospitals_tx_zip_merged.loc[closed_hospitals_tx_zip_merged['ZIP_CD'].isin(\n",
        "    indirectly_affected_zips_list), 'Closure Impact Status'] = 'Indirectly Affected'\n",
        "print(closed_hospitals_tx_zip_merged['Closure Impact Status'].value_counts())"
      ],
      "id": "a0cc0007",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create a color map for different impact status\n",
        "color_map = {\n",
        "    'Directly Affected': 'red',\n",
        "    'Indirectly Affected': 'orange',\n",
        "    'Not Affected': 'lightgray'\n",
        "}\n",
        "\n",
        "# plot a choropleth\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "closed_hospitals_tx_zip_merged.plot(\n",
        "    column='Closure Impact Status',\n",
        "    color=closed_hospitals_tx_zip_merged['Closure Impact Status'].map(\n",
        "        color_map),\n",
        "    edgecolor='0.8',\n",
        "    legend=True,\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "# create a custom legend\n",
        "patches = [mpatches.Patch(color=color, label=status)\n",
        "           for status, color in color_map.items()]\n",
        "ax.legend(handles=patches, title=\"Closure Impact Status\",\n",
        "          loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
        "\n",
        "ax.set_axis_off()\n",
        "plt.title('Impact Status of Hospital Closures in Texas (2016-2019)')\n",
        "plt.show()"
      ],
      "id": "fc5c29f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflecting on the exercise (10 pts) \n",
        "\n",
        "1. The “first-pass” method we’re using to address incorrectly identified closures in the data is imperfect. Can you think of some potential issues that could arise still and ways to do a better job at confirming hospital closures?\n",
        "\n",
        "There are some deficiencies in the method. In Section 2, when attempting to identify ‘false closures,’ we removed zip codes where the number of active hospitals in the closure year did not decrease in subsequent years. However, this approach only partially addresses the possibility of mergers. Since the count of active hospitals in the closure year does not include the hospitals that closed, the number may remain stable in the following year even if no merger or acquisition occurred. This means we may have mistakenly excluded some zip codes with actual closures. To improve, we could consider to compare the number of active hospitals in the year before the closure to the year after the closure. This would help us identify potential mergers more accurately, as it accounts for changes in hospital availability over a broader period. \n",
        "\n",
        "2. "
      ],
      "id": "9fb5b200"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "base",
      "language": "python",
      "display_name": "Python (base)",
      "path": "/Users/aa/Library/Jupyter/kernels/base"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}